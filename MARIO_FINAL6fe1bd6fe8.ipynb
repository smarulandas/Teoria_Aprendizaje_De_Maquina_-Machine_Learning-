{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "MARIO_FINAL6fe1bd6fe8",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smarulandas/Teoria_Aprendizaje_De_Maquina_-Machine_Learning-/blob/main/MARIO_FINAL6fe1bd6fe8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Desinstalar posibles conflictos\n",
        "!pip uninstall -y tensorflow tensorflow-probability tf-agents tf-agents-nightly\n",
        "\n",
        "# Instalar versiones compatibles\n",
        "!pip install tensorflow==2.13.0\n",
        "!pip install tensorflow-probability==0.21.0\n",
        "!pip install tf-agents==0.17.0\n",
        "!pip install 'imageio==2.4.0'\n",
        "!pip install pyvirtualdisplay pyglet swig\n",
        "!pip install gym[atari,box2d,accept-rom-license]\n",
        "!pip install gym-super-mario-bros"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:01:49.840725Z",
          "iopub.execute_input": "2025-07-19T21:01:49.841018Z",
          "iopub.status.idle": "2025-07-19T21:05:01.854182Z",
          "shell.execute_reply.started": "2025-07-19T21:01:49.840996Z",
          "shell.execute_reply": "2025-07-19T21:05:01.852524Z"
        },
        "id": "e_7wm-td5QzL"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "from gym.wrappers import GrayScaleObservation, ResizeObservation, FrameStack\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import imageio\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "from tf_agents.environments import gym_wrapper, tf_py_environment\n",
        "from tf_agents.networks.q_network import QNetwork\n",
        "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.policies import policy_saver\n",
        "\n",
        "# Iniciar display virtual para renderizado en video\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:05:01.856927Z",
          "iopub.execute_input": "2025-07-19T21:05:01.857381Z",
          "iopub.status.idle": "2025-07-19T21:05:14.169285Z",
          "shell.execute_reply.started": "2025-07-19T21:05:01.857342Z",
          "shell.execute_reply": "2025-07-19T21:05:14.168121Z"
        },
        "id": "xL8wukvy5QzQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomRewardMario(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(CustomRewardMario, self).__init__(env)\n",
        "        self.prev_x = 0\n",
        "        self.stuck_counter = 0\n",
        "\n",
        "    def reset(self, **kwargs):\n",
        "        self.prev_x = 0\n",
        "        self.stuck_counter = 0\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    def step(self, action):\n",
        "        obs, reward, done, info = self.env.step(action)\n",
        "        x_pos = info.get(\"x_pos\", 0)\n",
        "\n",
        "        # Penalizar retroceso significativo\n",
        "        if x_pos < self.prev_x - 5:\n",
        "            reward -= 10\n",
        "\n",
        "        # Penalizar estancamiento si no avanza\n",
        "        if abs(x_pos - self.prev_x) < 1:\n",
        "            self.stuck_counter += 1\n",
        "            if self.stuck_counter >= 15:\n",
        "                reward -= 10\n",
        "        else:\n",
        "            self.stuck_counter = 0\n",
        "\n",
        "        self.prev_x = x_pos\n",
        "        return obs, reward, done, info"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:05:14.170364Z",
          "iopub.execute_input": "2025-07-19T21:05:14.171074Z",
          "iopub.status.idle": "2025-07-19T21:05:14.179663Z",
          "shell.execute_reply.started": "2025-07-19T21:05:14.171048Z",
          "shell.execute_reply": "2025-07-19T21:05:14.178358Z"
        },
        "id": "dEVfS8pk5QzS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mario_env(version=\"v1\"):\n",
        "    env = gym_super_mario_bros.make(f\"SuperMarioBros-{version}\")\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "    env = CustomRewardMario(env)  # Wrapper que ajusta recompensas\n",
        "    env = GrayScaleObservation(env, keep_dim=True)\n",
        "    env = ResizeObservation(env, 84)\n",
        "    env = FrameStack(env, 4)\n",
        "    return gym_wrapper.GymWrapper(env)\n",
        "\n",
        "train_py_env = create_mario_env(\"v1\")\n",
        "eval_py_env = create_mario_env(\"v1\")\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:05:14.18278Z",
          "iopub.execute_input": "2025-07-19T21:05:14.183589Z",
          "iopub.status.idle": "2025-07-19T21:05:15.139584Z",
          "shell.execute_reply.started": "2025-07-19T21:05:14.183545Z",
          "shell.execute_reply": "2025-07-19T21:05:15.138685Z"
        },
        "id": "0l8K3KrH5QzU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "conv_layer_params = [(32, (8, 8), 4), (64, (4, 4), 2), (64, (3, 3), 1)]\n",
        "fc_layer_params = [512]\n",
        "preprocessing_layer = tf.keras.layers.Lambda(lambda obs: tf.cast(obs, tf.float32) / 255.0)\n",
        "\n",
        "q_net = QNetwork(\n",
        "    input_tensor_spec=train_env.observation_spec(),\n",
        "    action_spec=train_env.action_spec(),\n",
        "    preprocessing_layers=preprocessing_layer,\n",
        "    conv_layer_params=conv_layer_params,\n",
        "    fc_layer_params=fc_layer_params,\n",
        ")\n",
        "\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=2.5e-4, rho=0.95, momentum=0.0, epsilon=0.01)\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = DqnAgent(\n",
        "    time_step_spec=train_env.time_step_spec(),\n",
        "    action_spec=train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter,\n",
        "    gamma=0.99,\n",
        "    epsilon_greedy=0.1,\n",
        "    target_update_period=1000,\n",
        ")\n",
        "agent.initialize()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:05:15.140896Z",
          "iopub.execute_input": "2025-07-19T21:05:15.141259Z",
          "iopub.status.idle": "2025-07-19T21:05:15.757758Z",
          "shell.execute_reply.started": "2025-07-19T21:05:15.141227Z",
          "shell.execute_reply": "2025-07-19T21:05:15.756674Z"
        },
        "id": "ZtwVWLiY5QzV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
        "    data_spec=agent.collect_data_spec,\n",
        "    batch_size=train_env.batch_size,\n",
        "    max_length=100000,\n",
        ")\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(), train_env.action_spec())\n",
        "\n",
        "def collect_step(env, policy, buffer):\n",
        "    time_step = env.current_time_step()\n",
        "    action_step = policy.action(time_step)\n",
        "    next_time_step = env.step(action_step.action)\n",
        "    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
        "    buffer.add_batch(traj)\n",
        "\n",
        "# Rellenar el buffer inicialmente con acciones aleatorias\n",
        "for _ in range(2000):\n",
        "    collect_step(train_env, random_policy, replay_buffer)\n",
        "\n",
        "# Dataset de entrenamiento\n",
        "reward_history = []  # Lista para guardar recompensa media por iteración\n",
        "\n",
        "# Dataset desde el replay buffer\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3, sample_batch_size=32, num_steps=2\n",
        ").prefetch(3)\n",
        "iterator = iter(dataset)\n",
        "\n",
        "print(\"*----------Inicio de entrenamiento-----------\")\n",
        "# Bucle de entrenamiento\n",
        "for iteration in range(120000):\n",
        "    collect_step(train_env, agent.collect_policy, replay_buffer)\n",
        "    experience, _ = next(iterator)\n",
        "    train_loss = agent.train(experience).loss\n",
        "\n",
        "    # Registrar recompensa promedio del batch\n",
        "    reward = experience.reward.numpy().mean()\n",
        "    reward_history.append(reward)\n",
        "\n",
        "    # Mostrar progreso\n",
        "    if iteration % 500 == 0:\n",
        "        print(f\"🔁 Iteración {iteration}: pérdida = {train_loss:.4f}, recompensa promedio = {reward:.2f}\")\n",
        "\n",
        "checkpoint_dir = '/kaggle/working/mario_dqn_checkpoint512'\n",
        "policy_saver.PolicySaver(agent.policy).save(checkpoint_dir)\n",
        "print(f\"💾 Agente guardado en: {checkpoint_dir}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-19T21:05:15.758903Z",
          "iopub.execute_input": "2025-07-19T21:05:15.759205Z"
        },
        "id": "74vXw1z05QzX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def plot_rewards(reward_history, smooth=50):\n",
        "    rewards = np.array(reward_history)\n",
        "    if smooth > 1:\n",
        "        kernel = np.ones(smooth) / smooth\n",
        "        rewards = np.convolve(rewards, kernel, mode=\"valid\")\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(rewards)\n",
        "    plt.title(\"Recompensa promedio por iteración (suavizada)\")\n",
        "    plt.xlabel(\"Iteración\")\n",
        "    plt.ylabel(\"Recompensa\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "plot_rewards(reward_history)"
      ],
      "metadata": {
        "trusted": true,
        "id": "MwTdtqpq5QzZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import imageio\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import gym_super_mario_bros\n",
        "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "def record_mario_video(agent, max_steps=500, fps=30, filename='mario_fixed512.mp4'):\n",
        "\n",
        "    env = gym_super_mario_bros.make(\"SuperMarioBros-v1\")  # SIN render_mode\n",
        "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
        "\n",
        "    obs = env.reset()\n",
        "    dummy_time_step = train_env.reset()\n",
        "    frames = []\n",
        "    total_reward = 0\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        try:\n",
        "            frames.append(env.screen.copy())\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        obs_proc = np.expand_dims(obs, axis=0).astype(np.uint8)\n",
        "        obs_proc = tf.image.rgb_to_grayscale(obs_proc)\n",
        "        obs_proc = tf.image.resize(obs_proc, (84, 84))\n",
        "        obs_proc = tf.cast(obs_proc, tf.uint8)\n",
        "        obs_proc = tf.stack([obs_proc] * 4, axis=1)\n",
        "\n",
        "        time_step = dummy_time_step._replace(observation=obs_proc)\n",
        "        action_step = agent.policy.action(time_step)\n",
        "        action = int(action_step.action.numpy()[0])\n",
        "\n",
        "        obs, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    output_path = f\"/kaggle/working/{filename}\"\n",
        "    imageio.mimsave(output_path, frames, fps=fps)\n",
        "    print(f\"🎥 Video guardado en: {output_path} | Recompensa total: {total_reward:.2f}\")\n",
        "    return output_path\n",
        "\n",
        "# Grabar y Guardar el video\n",
        "video_path = record_mario_video(agent, max_steps=1500, filename=\"mario_v1_episode512.mp4\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "A2MLAsU95Qzb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}